# OpenCV 12_机器学习

## 1. 机器学习基本概念

### 机器学习基本概念

机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。

- 机器学习的对象是数据，数据的基本假设是同类数据具有一定的统计规律性。机器学习的目的是对数据进行预测和分析。

- 机器学习分为监督学习，无监督学习和强化学习等方法。

  > 机器学习从给定的，有限的，用于学习的训练数据集出发，假设数据为独立同分布，并假设需要学习的模型属于某个函数的集合，称为假设空间，应用某个评价准则，从假设空间中选取一个最优模型，使得它对一致的训练数据集和测试数据集在给定的评价准则下有最优的预测。
  >
  > **机器学习的三要素**：模型，策略和算法（模型的假设空间，模型的选择准则，模型学习的算法）


- 机器学习的步骤：

  > 1. 得到有限的训练数据集；
  > 2. 确定包含所有可能的模型的假设空间，即学习模型的集合；
  > 3. 确定模型选择准则（策略）；
  > 4. 实现求解最优模型的算法（算法）；
  > 5. 通过学习方法确定最优模型；
  > 6. 使用得到的最优模型对新数据进行预测和分析。

### 机器学习的分类

#### 监督学习

- 监督学习是指从标注数据中学习预测模型的机器学习问题，其本质是学习**输入到输出的映射的统计规律**。

> - 输入空间：输入的所有可能取值的集合；
> - 实例：每一个具体的输入，通常由特征向量进行表示；（此特征代数非特征向量中的特征向量）
> - 特征空间：所有特征向量存在的空间，模型定义在特征空间上；
> - 输出空间：输出所有可能取值的集合。

> - 回归问题：输入变量和输出变量均为连续变量的预测问题；
> - 分类问题：输出变量为有限个离散变量的预测问题；
> - 标注问题：输入变量和输出变量均为变量序列的预测问题。

输入变量用 $X$ 表示，输入变量取值用 $x$ 表示；输出变量用 $Y$ 表示，输出变量的取值用 $y$ 表示。
$$
 \bold{x} = (x^{(1)},x^{(2)},\cdots,x^{(n)})^T 
$$
样本容量为 N 的训练集可以表示为：
$$
T = \{(\bold{x}_1,y_1),(\bold{x}_2,y_2),\cdots,(\bold{x}_N,y_N)\}
$$

- 监督学习的基本假设：$X$ 和 $Y$ 有联合概率分布 $P(X,Y)$；

- 监督学习的目的：学习一个输入到输出的映射，该映射用模型（条件概率 $P(Y|X)$ 或者决策函数 $Y = f(X)$）表示。
- 监督学习分为**学习**和**预测**两个过程：

![NULL](./assets/picture_1.jpg)

> 学习系统利用给定的训练数据集，通过学习或者训练得到一个模型；
>
> 预测系统对于给定的测试样本集中的输入 $x_{N+1}$ ，由模型 $y_{N+1} = argmax\hat{P}(y|x_{N+1})$或者$y_{N+1}=\hat{f}(x_{N+1})$给出相应的输出。

#### 无监督学习

- 无监督学习是指从无标注数据（自然得到的数据）中学习预测模型的机器学习问题，其本质是**学习数据中的统计规律或者潜在结构**。

> - 输入空间：输入的所有可能取值的集合；
> - 实例：每一个具体的输入，通过特征向量表示；
> - 输出空间：输出所有可能取值的集合。输出是对输入的分析结果（不是映射），通过输入的类别/概率进行表示。

- 无监督学习可以进行数据的聚类/概率估计。

样本容量为 N 的训练集可以表示为：
$$
U = \{x_1,x_2,\cdots,x_N\}
$$


- 无监督学习的目的：选出在给定评价标准下最优的模型，该模型条件概率分布 $P(z|x)$，$P(x|z)$ 或者函数 $z = g(x)$ 表示。

- 无监督学习分为学习和预测两个过程：

![NULL](./assets/picture_2.jpg)

> 学习系统利用给定的训练数据集，通过学习或者训练得到一个模型；
>
> 预测系统对于给定的测试样本集中的输入 $x_{N+1}$ ，由模型 $y_{N+1} = argmax\hat{P}(y|z_{N+1})$或者$z_{N+1}=\hat{g}(z_{N+1})$给出相应的输出，进行聚类或者概率估计。

### 机器学习的要素

#### 模型

假设空间：所有可能的条件概率分布或者决策函数的集合。

- 如果是决策函数的集合：$\{f|Y=f(X)\}$，则该集合由一个参数向量$\bold{\theta}$决定的函数族构成。参数向量构成参数空间。
- 如果是条件概率的集合：$\{P|P(Y|X)\}$，则该集合由一个参数向量$\bold{\theta}$决定的条件概率分布族构成。参数向量构成参数空间。

#### 策略

- 损失函数$L(Y,f(X))$：衡量模型一次预测的好坏；

> - 风险函数$R_{exp}(f)$：衡量平均意义下模型预测的好坏；
>
> $$
> R_{exp}(f) = 
> \int L(y,f(x))P(x,y)dxdy
> $$
>
> - 经验风险$R_{emp}(f)$：衡量模型$f(X)$关于训练数据集的平均损失。
>
> $$
> R_{emp}(f) = 
> \frac{1}{N}\sum^N_{i_1}L(y_i,f(x_i))
> $$

> - 常见的损失函数：
>
> 1. 0-1 损失函数：
>
> $$
> L(Y,f(X)) = 
> \left\{  
> \begin{array}{**lr**}  
> 0,& Y = f(X)		\\
> 1,& Y \neq f(X)
> \end{array}  
> \right.
> $$
>
> 2. 平方损失函数：
>
> $$
> L(Y,f(X)) =  (Y-f(X))^2
> $$
>
> 3. 绝对损失函数：
>
> $$
> L(Y,f(X)) = |Y-f(X)|
> $$
>
> 4. 对数损失函数：
>
> $$
> L(Y,P(Y|X)) = -logP(Y|X)
> $$

- 策略：选定损失函数，使得损失函数某一个指标最小化。

> 1. 经验风险最小化：样本容量足够大时，经验风险最小化能保证有比较好的学习效果。模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计法。样本容量很小时容易产生过拟合。
> 2. 结构风险最小化（正则化）：在经验风险上加入正则化项（罚项）以表达模型复杂度。
>
> $$
> R_{srm}(f) = \frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)
> $$
>
> 式中$J(f)$为假设空间内的泛函，衡量复杂度，$\lambda$用于加权。

#### 算法

- 算法：求解最优模型，通常采用迭代方法。

### 模型评估和模型选择

#### 训练误差和测试误差

- 训练误差是模型 $Y=\hat f(X)$ 关于训练数据集的平均损失：

$$
R_{srm}(f) = \frac{1}{N}\sum^N_{i=1}L(y_i,\hat f(x_i))
$$

- 测试误差是模型 $Y=\hat f(X)$ 关于测试数据集的平均损失：

$$
e_{test} = \frac{1}{N^,}\sum^{N^,}_{i=1}L(y_i,\hat f(x_i))
$$

训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。测试误差反映了学习方法对未知的测试数据集的预测能力，是学习中的重要概念。显然，给定两种学习方法，测试误差小的方法具有更好的预测能力，是更有效的方法。通常将学习方法对未知数据的预测能力称为泛化能力。

#### 过拟合

当假设空间含有不同复杂度(例如，不同的参数个数)的模型时，就要面临模型选择的问题。我们希望选择或学习一个合适的模型。如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型。具体地，所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高。这种现象称为过拟合。**过拟合是指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。**可以说模型选择旨在**避免过拟合并提高模型的预测能力**。

#### 正则化

模型选择的典型方法是正则化。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项/罚项。

正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如模型参数向量的范数。

**在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。**

从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。

#### 交叉验证

如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集、验证集和测试集。

训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。

在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。

但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。

1. 简单交叉验证

简单交叉验证方法是：首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集(例如，70%数据为训练集，30%数据为测试集)；然后用训练集在各种条件下(例如，不同的参数个数)训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。

2. S折交叉验证

首先随机地将已给数据切分为S个互不相交、大小相同的子集；然后利用 S-1 个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的 S 种选择重复进行；最后选出 S 次评测中平均测试误差最小的模型。

3. 留一交叉验证

S折交叉验证的特殊情形是$S = N$,称为留一交叉验证，往往在数据缺乏的情况下使用。这里，$N$是给定数据集的容量。

#### 泛化能力

学习方法的泛化能力是指由该方法学习到的模型对未知数据的预测能力。

泛化误差：如果学到的模型是 $\hat f$,那么用这个模型对未知数据预测的误差即为泛化误差。
$$
R_{exp}(\hat f) = \int L(y,\hat f(x))P(x,y)dxdy
$$
泛化误差反映了学习方法的泛化能力，如果一种方法学习的模型比另一种方法学习的模型具有更小的泛化误差，那么这种方法就更有效。事实上，泛化误差就是所学习到的模型的期望风险。

泛化误差上界是样本容量的函数，当样本容量增加时，泛化上界趋于0；它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

## 2. 监督学习

#### 线性回归

