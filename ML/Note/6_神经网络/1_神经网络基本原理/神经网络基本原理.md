# 1_神经网络基本原理

> 参考链接：
>
> [3Blue1Brown 的视频](https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.1387.homepage.video_card.click&vd_source=2d2507d13250e2545de99f3c552af296)
>
> [阿岳的视频](https://www.bilibili.com/video/BV1Vj411P7N6/?spm_id_from=333.1387.homepage.video_card.click&vd_source=2d2507d13250e2545de99f3c552af296)

> 神经网络是一种模仿人脑处理信息方式的计算模型，它由许多相互连接的节点（神经元）组成，这些节点按层次排列。神经网络的强大之处在于其能够自动从大量数据中学习复杂的模式和特征，无需人工设计特征提取器。

> 神经网络是深度学习的一个重要部分。

## 1. 神经网络基本原理

### 网络结构

神经网络的运行结果从外部看来类似于：给定了一堆输入，通过一个黑箱，形成输出：

<img src="./assets/picture_1.jpg" alt="NULL" style="zoom:50%;" />

以数字识别为例，给定了一个图像(每个像素灰度值是一个输入)，通过这个"黑箱"，输出识别的结果。接下来讨论如何识别出来图像中的数字。

神经网络类比了生物学中神经元连接的方式，通过上一层"神经元"激活下一层"神经元"，层层激活，最后激活输出层，对于数字识别而言，每一个输出代表这张图像是哪个数字的可能性，可能性最大的就是神经网络的识别结果：

<img src="./assets/picture_2.jpg" alt="NULL" style="zoom:50%;" />

这里简要的说明这种层状的连接结构如何实现"识别"这个功能的，本质上，神经网络能够实现**对目标特征的拆解**（特征工程）。

比如数字可以拆成"横"，"竖"，"圆圈"的特征：

![NULL](./assets/picture_3.jpg)

所以，神经网络的倒数第二层更希望能够识别出这些特征，当识别到这些被拆解的特征时，对应的神经元会被更强的激活，此时从倒数第二层到输出层的任务就是学习哪些组件能够组合出哪些数字：

<img src="./assets/picture_4.jpg" alt="NULL" style="zoom:50%;" />

而这些被拆解的特征在上一层中可以被拆解成更细微的特征：

<img src="./assets/picture_5.jpg" alt="NULL" style="zoom:50%;" />

以此类推，最后可以得知，神经网络通过识别各个小特征，然后激活识别到大的特征，最后根据大的特征输出最终的识别结果：

<img src="./assets/picture_6.jpg" alt="NULL" style="zoom:50%;" />

但是神经网络可能学习得到的特征并不是我们想象到的特征，很多时候都难以得知神经网络究竟学习到了什么样的特征。

### 正向传播

正向传播就是神经网络的"激发"过程。

首先会对每一条神经元之间的连线定义一个权重 $w$，表征上一层神经元的激活值对下一层神经元的激活值的影响程度。下一层神经元会将连接的上一层神经元的激活值通过加权相加得到自身的激活值(原始值，未加入偏置和激活函数处理)，同时，下一层神经元为控制自身的激活程度，通常需要加入一个偏置。

同时网络中对神经元的激活程度进行处理，比如激活程度小，神经元可以选择不激活；激活程度大，神经元可以得到一个大的激活值，这时通常需要一个激活函数对神经元加权得到的值进行处理。

由此正向传播的过程如下：

![NULL](./assets/picture_7.jpg)

> ***激活函数***：激活函数可以分为饱和和非饱和两类。使用激活函数能够给神经元引入非线性因素，使得**神经网络可以任意逼近任何非线性函数**，使深层神经网络表达能力更加强大。
>
> **不使用激活函数会导致神经网络退化为多层感知机，只能拟合线性模型。**
>
> 参考链接：[链接](https://blog.csdn.net/qq_42691298/article/details/126590726)
>
> > - **饱和激活函数：**`sigmoid`、`tanh` ...
> > - **非饱和激活函数:**`ReLU`、`Leaky Relu`、`ELU`、`PReLU`、`RReLU` ...
> >
> > 非饱和激活函数能解决深度神经网络（层数非常多）带来的梯度消失问题，而且使用非饱和激活函数能加快收敛速度。
>
> 1. `sigmoid` 函数
>    $$
>    f(x) = \frac{1}{1+e^{-x}}\\
>    f^,(x) = f(x)(1-f(x))
>    $$
>    ![NULL](./assets/picture_8.jpg)
>
> 2. `tanh` 函数
>    $$
>    f(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} = 2sigmoid(2x)-1
>    $$
>    ![NULL](./assets/picture_9.jpg)
>
> 3. `ReLU` 函数
>    $$
>    f(x) = \max(0,x)
>    $$
>    ![NULL](./assets/picture_10.jpg)
>
> 4. `Leaky Relu` 函数
>    $$
>    f(x;\alpha) = \max(\alpha x,x)
>    $$
>    ![NULL](./assets/picture_11.jpg)
>
> 5. `PRelu` 函数
>    $$
>    f(x;\alpha)=
>    \begin{equation*}
>    \begin{cases}
>    \alpha x , x \textless 0 \\
>    x,x\geq 0 
>    \end{cases}
>    \end{equation*}
>    $$
>    $\alpha$ 通过学习得到。
>
>    ![NULL](./assets/picture_12.jpg)
>
> 6. `ELU` 函数
>    $$
>    f(x;\alpha)=
>    \begin{equation*}
>    \begin{cases}
>    \alpha(e^x-1), x \leq 0 \\
>    x,x \textgreater 0
>    \end{cases}
>    \end{equation*}
>    $$
>    ![NULL](./assets/picture_13.jpg)
>
> 7. `SELU` 函数
>    $$
>    f(x;\alpha)=
>    \begin{equation*}
>    \begin{cases}
>    \lambda\alpha(e^x-1), x \leq 0 \\
>    \lambda x,x \textgreater 0
>    \end{cases}
>    \end{equation*} \\
>    \alpha = 1.6733 \\
>    \lambda = 1.0507
>    $$
>    ![NULL](./assets/picture_14.jpg)
>
> 8. `Swish` 函数
>    $$
>    f(x) = x sigmoid(x)
>    $$
>    ![NULL](./assets/picture_15.jpg)
>
> 9. `Mish` 函数
>    $$
>    f(x) = x tanh(ln(1+e^x))
>    $$
>    ![NULL](./assets/picture_16.jpg)
>
> 10. `softmax` 函数
>     $$
>     f(x) = \frac{e^{x_i}}{\sum e^{x_i}}
>     $$
>     ![NULL](./assets/picture_17.jpg)

神经网络训练的目标是寻找出合适的**权重 $w$** 和**偏置 $b$** 使得损失函数最小化。

### 权重调整

最初的时候权重是随机分配的，当然，随机分配需要遵循特定的规则，比如一个神经元有 $n$ 条线和上一层神经元连接，那么每条线上的权重在 $-\frac{1}{\sqrt n }$ 和 $\frac{1}{\sqrt n }$ 之间随机分配；或者使用高斯分布的方法进行随机分配。

训练过程中，对一组输入，总会对应着一组神经网络的输出和标准的输出：

<img src="./assets/picture_18.jpg" alt="NULL" style="zoom:50%;" />

神经网络的输出和标准输出总存在差别，通常需要定义损失函数 $J$（比如对应输出层的神经元1，定义损失函数 $\frac{1}{2}(y_1-\hat y_1)^2$），训练的目标就是使得 $J$ 最小。

首次以下面这个简单的神经网络为例子，当1层神经元输入 $x$ 时，输出为 $\hat y$，但是期望的输出为 $y$。

<img src="./assets/picture_19.jpg" alt="NULL" style="zoom:50%;" />

这时并不能直接改变激活值 $x$，只能尝试改变权重 $w$ 和偏置 $b$ 来实现损失函数 $J(w,b)$ 的最小化。
$$
J(w,b) = \frac{1}{2} (y-\hat y)^2 = \frac{1}{2}(f(wx+b)-\hat y)^2
$$

> 此时损失函数和 $\hat y$ 并没有关系。

这时可以采用梯度下降法优化 $w,b$。
$$
w \leftarrow w - \lambda\nabla_wJ(w,b) \\
b \leftarrow b - \lambda\nabla_bJ(w,b) \\
$$
如果神经元有多个输入，做法仍然是一致的：通过损失函数(用 $w,b,\hat y$ 表示)计算得到关于 $w,b$ 的梯度，乘以一个学习率，实现梯度下降。这样能实现对该神经元和上一层神经元所有连线的权重和偏置优化。

### 误差反向传播

对于没有隐藏层的神经网络，权重调整是很容易的，因为输出层的目标值是知道的。但是对于多层神经网络，隐藏层的损失是多少，目标是多少还是未知的，如何优化权重和偏置，这就是反向传播需要解决的另一个问题（误差反向传播）。

<img src="./assets/picture_20.jpg" alt="NULL" style="zoom:50%;" />

> 反向传播**先进行误差的反向传播，再按照各层的误差同时优化各层权重**。

以下图为例：

<img src="./assets/picture_21.jpg" alt="NULL" style="zoom:50%;" />

首先按连线上的权重分割误差：
$$
e_1^{(3)} = \frac{w_{11}^{(23)}}{w_{11}^{(23)}+w_{21}^{(23)}}e_1^{(2)'} + \frac{w_{21}^{(23)}}{w_{11}^{(23)}+w_{21}^{(23)}}e_2^{(2)'} \\
e_2^{(3)} = \frac{w_{12}^{(23)}}{w_{12}^{(23)}+w_{22}^{(23)}}e_1^{(2)''} + \frac{w_{22}^{(23)}}{w_{12}^{(23)}+w_{22}^{(23)}}e_2^{(2)''}
$$
上一层的神经元会得到连线上分配的误差，将这些误差相加可以得到该神经元的误差：
$$
e_1^{(2)} = e_1^{(2)'} + e_1^{(2)''} \\
e_2^{(2)} = e_2^{(2)'} + e_2^{(2)''}
$$
由此可以进行误差反向传播。

但是在计算机中做除法会花费很大的计算开销，实际上可以去掉分母，实现的效果是类似的（只不过是失去了归一化的效果）。

### 反向传播

实际上，反向传播的本质是**偏导数的链式法则**（因为本质上就是对各个权重和偏置求偏导）。接下来从数学方面解释如何进行优化：

<img src="./assets/picture_22.jpg" alt="NULL" style="zoom:50%;" />

- 损失函数：$J=\frac{1}{2}(y-a_1^{(3)})^2$；

- 求隐藏层到输出层的权重梯度：

  $w_{11}^{(23)}$ 的梯度：$\frac{\partial J}{\partial w_{11}^{(23)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial w_{11}^{(23)}}$；

  $w_{21}^{(23)}$ 的梯度：$\frac{\partial J}{\partial w_{21}^{(23)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial w_{21}^{(23)}}$；
  
  $b_1^{(3)}$ 的梯度：$\frac{\partial J}{\partial b_{1}^{(3)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial b_{1}^{(3)}}$。
  
- 求输入层到隐藏层的权重梯度：

  $w_{11}^{(12)}$ 的梯度：$\frac{\partial J}{\partial w_{11}^{(12)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{1}^{(2)}}\frac{\partial a_1^{(2)}}{\partial z_{1}^{(2)}}\frac{\partial z_1^{(2)}}{\partial w_{11}^{(12)}}$；
  
  $w_{21}^{(12)}$ 的梯度：$\frac{\partial J}{\partial w_{21}^{(12)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{1}^{(2)}}\frac{\partial a_1^{(2)}}{\partial z_{1}^{(2)}}\frac{\partial z_1^{(2)}}{\partial w_{21}^{(12)}}$；
  
  $w_{12}^{(12)}$ 的梯度：$\frac{\partial J}{\partial w_{12}^{(12)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{2}^{(2)}}\frac{\partial a_2^{(2)}}{\partial z_{2}^{(2)}}\frac{\partial z_2^{(2)}}{\partial w_{12}^{(12)}}$；
  
  $w_{11}^{(22)}$ 的梯度：$\frac{\partial J}{\partial w_{22}^{(12)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{2}^{(2)}}\frac{\partial a_2^{(2)}}{\partial z_{2}^{(2)}}\frac{\partial z_2^{(2)}}{\partial w_{22}^{(12)}}$；
  
  $b_1^{(2)}$ 的梯度：$\frac{\partial J}{\partial b_{1}^{(2)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{1}^{(2)}}\frac{\partial a_1^{(2)}}{\partial z_{1}^{(2)}}\frac{\partial z_1^{(2)}}{\partial b_{1}^{(2)}}$；
  
  $b_2^{(2)}$ 的梯度：$\frac{\partial J}{\partial b_{2}^{(2)}} = \frac{\partial J}{\partial a_1^{(3)}}\frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}\frac{\partial z_1^{(3)}}{\partial a_{2}^{(2)}}\frac{\partial a_2^{(2)}}{\partial z_{2}^{(2)}}\frac{\partial z_2^{(2)}}{\partial b_{2}^{(2)}}$。

> 误差实际上定义为 $\frac{\partial J}{\partial z}$，实际上可以看到，误差反向传播的过程就是去掉分母的按权重分配误差。

总体概括神经网络的训练流程：**正向传播->反向传播更新权重（其中包含了误差的传播）**。

## 2. 神经网络的一些问题

### 正则化

1. L1/L2 正则化

   L1/L2 正则化在机器学习概述中已有说明，通常神经网络的训练会选取 L2 正则化。

   L1 正则化项：
   $$
   \lambda \sum||w||
   $$
   L2 正则化项：
   $$
   \lambda\sum\frac{1}{2}||w||^2
   $$

2. 暂退法 Dropout

   简单的模型从另一方面来讲就是平滑性：函数不应该对其输入的微小变化敏感。由此，可以在训练过程中，在计算后续层之前向网络的每一层注入噪声，注入噪声只会在输入-输出映射上增强平滑性。其中一种注入噪声的方式就是 Dropout。

   **Dropout 在训练阶段时使用，测试和预测时停用。**

   1. 随机选择：在每个训练样本的前向传播开始前，以一定的概率 $p$ 随机选择网络中的神经元（通常是隐藏层神经元），并将其暂时关闭。

      随机过程：根据概率生成随机掩码 $m_i \sim Bernoulli(p)$，该掩码决定是否关闭每个神经元。被保留神经元的输出需要进行缩放补偿，缩放比例为 $\frac{m_i}{1-p}$；

      对于隐藏层神经元，概率通常设为 0.5。对于输入层，通常设为 0.1 或 0.2，因为丢弃太多输入信息可能不利于学习。网络神经元越多、层数越深，或者训练数据越少，过拟合风险越高，可以适当使用更高的丢弃率。

   2. 前向与反向传播：只在这个被精简过的子网络上进行本次训练的前向传播和反向传播，更新仍然活跃的神经元的权重。

   3. 恢复与重复：为下一个训练样本恢复所有神经元，然后再次随机丢弃一部分，重复上述过程。

   ![NULL](./assets/picture_23.jpg)

3. 早停法

   开始时,将训练的数据分为训练集和验证集，每次训练结束后（或每 N 次训练结束后)，在验证集上获取测试结果，记录目前为止最好的验证集精度, 而随着训练轮次的增加，如果在验证集上发现测试误差上升，则停止训练，将之前处理测试集时准确率最高时的权重作为网络的最终参数。

   ![NULL](./assets/picture_24.jpg)

   > **停止标准:**
   >
   > - GL 泛化损失：如果当前的验证损失比之前的最佳损失高出太多，说明模型的泛化能力已经显著下降，应该停止训练。
   >   $$
   >   GL(t) = 100(\frac{L(t)}{L_{min}(t)}-1)
   >   $$
   >   $L(t)$ 为第 $t$ 个 Epoch 后的验证集损失，$L_{min}(t)$ 为从第 1 到第 $t$ 个 Epoch 中，最小的验证集损失。当 $GL(t)$ 超过一个预定义的阈值 $\alpha$ 时，即认为泛化损失过大，触发停止。
   >
   > - UP 无用进度：如果模型在连续多个 Epoch 内都无法刷新最佳记录，那么继续训练下去可能只是在浪费时间，应该停止。
   >   $$
   >   UP(t) = 1000 (\frac{\min\{L(k)|k=t-s,\cdots,t\}}{L_{min}(t-s)}-1)
   >   $$
   >   $s$ 为窗口大小超参数，$L_{min}(t-s)$ 为在窗口开始时的历史最佳损失。$\min\{L(k)|k=t-s,\cdots,t\}$ 为在窗口内能到达的最佳损失。当 $UP(t)$ 超过一个预定义的阈值 $\beta$ 时，即认为近期毫无进展，触发停止。
   >
   > - PQ ：结合 GL 和 UP，只有当模型既出现了显著的泛化损失（GL），又在近期没有取得任何进展（UP） 时，才认为训练应该被终止。

4. 数据增强

   较少的样本数和较少的特征通常会造成过拟合，通过合适的预处理方式和扩大样本数可以减少过拟合。

### 常用的损失函数

#### 回归损失函数

1. **均方误差 MSE**
   $$
   J = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
   $$
   MSE 对异常值敏感，梯度随误差增大而增大。

2. **平均绝对误差 MAE/L1 Loss**
   $$
   J = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|
   $$
   MAE 对异常值不敏感，梯度恒定，即使对于小的损失值，其梯度也是大的，不利于函数的收敛和模型的学习。

3. **Huber Loss**
   $$
   J = 
   \begin{cases}
   0.5(y-\hat y )^2 & |y-\hat y| < \delta \\
   \delta|y-\hat y|-\frac{1}{2}\delta^2 & |y-\hat y| > \delta
   \end{cases}
   $$
   超参数 $\delta$ 决定了 Huber Loss 对 MSE 和 MAE 的侧重性，当 $|y-\hat y| < \delta$ 时，变为 MSE；当 $|y-\hat y| > \delta$ 时，则变成类似于 MAE，因此 Huber Loss 同时具备了 MSE 和 MAE 的优点，减小了对离群点的敏感度问题，实现了处处可导的功能。

   ![NULL](./assets/picture_25.jpg)

#### 分类损失函数

1. **交叉熵**
   $$
   -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})
   $$
   $C$ 为类别数，$y_{i,c}$ 为符号函数，代表分类类别，$\hat{y_{i,c}}$ 为得到的预测概率。$C = 2$ 时为二元交叉熵。

### 其他问题

1. 梯度爆炸和梯度消失：

   常用的 `sigmoid` 和 `tanh` 激活函数的梯度小于1，在深度网络中，经过多个层的反向传播后，梯度会乘以这些小的导数值，导致梯度指数级减小。

   如果权重初始化过小，乘积效应会导致梯度在传播过程中迅速减小。如果权重初始化过大，那么在反向传播过程中，梯度的计算会受到很大的影响，容易导致梯度爆炸。

   如果学习率设置得过高，那么模型参数在更新时可能会因为步长过大而跳出最优解的范围。同时，过高的学习率会使模型在更新参数时过于激进，从而加剧梯度的波动，导致梯度爆炸。

   在非常深的网络中，梯度必须通过多个层传播。每经过一个层，梯度都会乘以该层的权重导数。如果这些乘积都很小，最终梯度会趋向于零；如果每一层的梯度都稍微增大一点，那么经过多层传播后，梯度值就会变得非常大，从而导致梯度爆炸。网络层数的增加会加剧梯度消失/爆炸的风险。

   解决方法：

   > 1. 替换激活函数：非饱和激活函数解决了梯度消失/爆炸问题。
   > 2. 合理的权重初始化方法；正则化。
   > 3. 梯度剪切：在每个训练步骤后检查梯度的范数（或某些权重的范数），如果超过了某个阈值，就将梯度进行缩放，从而限制梯度的大小。
   > 4. 批量归一化：在每层的输入上对数据进行标准化来减少梯度变化。对当前小批次的均值和方差进行归一化，使得每一层的输入分布保持稳定。这样可以有效缓解内部协变量漂移现象，即每一层的输入分布随着网络参数的更新而发生变化的现象。批归一化不仅有助于提高网络的训练速度和稳定性，还可以在一定程度上缓解梯度爆炸问题。
   > 5. 学习率调整，使用自适应学习率优化器；
   > 6. 使用更简洁的网络结构。

2. 对称性和参数初始化

   在隐藏层中，多个神经元的参数可以是对称的（即相同的），导致这些神经元在前向传播时计算出完全相同的值。在反向传播过程中，它们的梯度也是相同的。这种对称性导致网络的多个神经元实际上只发挥一个神经元的作用，从而极大地降低了网络的表达能力。

   合理的权重初始化可以打破对称性。

   > **参数初始化方法：**
   >
   > 1. Xavier 初始化：保持前向传播和反向传播中信号的方差一致。
   >
   >    - 均匀分布：$W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}\right]$
   >    - 正态分布：$W \sim N\left(0, \sqrt{\frac{2}{n_{in} + n_{out}}}\right)$
   >
   >    $n_{in}$ 为输入神经元数量，$n_{out}$ 为输出神经元数量。适用与 `sigmoid` 和 `tanh` 等对称激活函数。
   >
   > 2. He 初始化：
   >
   >    - 均匀分布：$W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{in}}}, \frac{\sqrt{6}}{\sqrt{n_{in}}}\right]$
   >    - 正态分布：$W \sim N\left(0, \sqrt{\frac{2}{n_{in}}}\right)$
   >
   >    主要针对 `ReLU` 及其变体。

