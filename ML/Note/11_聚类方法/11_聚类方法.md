# ML 11_聚类方法

聚类是针对给定的样本，依据它们特征的相似度或距离，将其归并到若干个类的数据分析问题。一个类是给定样本集合的一个子集。直观上，相似的样本聚集在相同的类，不相似的样本分散在不同的类。

聚类属于无监督学习，因为只是根据样本的相似度或距离将其进行归类，而类或簇事先并不知道。

## 1. 聚类的基本概念

- 相似度和距离

  聚类的对象是观测数据，或样本集合。假设有 $n$ 个样本，每个样本由 $m$ 个属性的特征向量组成。样本集合可以用矩阵 $X$ 表示。
  $$
  X = [x_{ij}]_{m\times n}
  $$
  矩阵的第 $j$ 列表示第 $j$ 个样本，第 $i$ 行表示第 $i$ 个属性。

  聚类的核心概念是相似度或距离，有多种相似度或距离的定义。因为相似度直接影响聚类的结果，所以其选择是聚类的根本问题。在聚类中，可以将样本集合看作是向量空间中点的集合，以该空间的距离表示样本之间的相似度。

  1. 闵可夫斯基距离
     $$
     d_{ij} = (\sum_{k=1}^m|x_{ki}-x_{kj}|)^{\frac{1}{p}}
     $$
     为样本 $x_i$ 和 $x_j$ 之间的闵可夫斯基距离。当 $p=2$ 时称为欧氏距离，$p=1$ 时称为曼哈顿距离，$p=\infin $ 时称为切比雪夫距离。

     闵可夫斯基距离越大相似度越小，距离越小相似度越大。

  2. 马哈拉诺比斯距离

     马哈拉诺比斯距离考虑各个分量(特征)之间的相关性并与各个分量的尺度无关。
     $$
     d_{ij} = ((x_i-x_j)^TS^{-1}(x_i-x_j))^{\frac{1}{2}}
     $$
     $S$​ 为协方差矩阵。马哈拉诺比斯距离越大相似度越小，距离越小相似度越大。

  3. 相关系数
     $$
     r_{ij} = \frac{\sum_{k=1}^m (x_{ki}-\overline x_i )(x_{kj}-\overline x_j )}{(\sum_{k=1}^m (x_{ki}-\overline x_i )^2 \sum_{k=1}^m (x_{kj}-\overline x_j )^2)^{\frac{1}{2}}}
     $$
     相关系数的绝对值越接近于1，表示样本越相似；越接近于0，表示样本越不相似。

- 类或簇

  通过聚类得到的类或簇，本质是样本的子集。如果一个聚类方法假定一个样本只能属于一个类，或类的交集为空集，那么该方法称为**硬聚类方法**。否则，如果一个样本可以属于多个类，或类的交集不为空集，那么该方法称为**软聚类方法**。

  > 设 $T$ 为给定的正数，若集合 $G$ 中任意两个样本 $x_i$ ，$x_j$，有 $d_{ij} \leq T$，则 $G$ 为一个类或簇。

  1. 类的均值 $\overline x_G$，又称为类的中心：
     $$
     \overline x_G  =\frac{1}{n_G}\sum_{i=1}^{n_G}x_i
     $$

  2. 类的直径 $D_G$，类中任意两个样本之间的最大距离。

  3. 类的样本散布矩阵 $A_G$ 与样本协方差矩阵 $S_G$。

- 类的距离

  1. 最短距离或单连接：样本之间的最短距离为两类之间的距离；
  2. 最长距离或完全连接：样本之间的最长距离为两类之间的距离；
  3. 中心距离：$\overline x_p$ 与 $\overline x_q$ 之间的距离为两类之间的距离；
  4. 平均距离：任意两个样本之间距离的平均值为两类之间的距离。

## 2. 层次聚类

层次聚类假设类别之间存在层次结构，将样本聚到层次化的类中。

层次聚类又有聚合或自下而上聚类、分裂或自上而下聚类两种方法。

> 聚合聚类开始将每个样本各自分到一个类；之后将相距最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化的类别。
>
> 分裂聚类开始将所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件；得到层次化的类别。

- 聚合聚类算法：
  1. 计算 $n$ 个样本两两之间的欧氏距离，记作矩阵$D=[d_{ij}]$。
  2. 构造 $n$ 个类，每个类只包含一个样本。
  3. 合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。
  4. 计算新类与当前各类的距离。若类的个数为1，终止计算，否则回到3。

## 3. K 均值聚类(K-Means)

1. 随机选择 $K$ 个点$\mu_1,\mu_2,...,\mu_K$作为聚类中心；
2. 对于数据集中的 $N$ 个点 $x_1,x_2,...,x_N$，分别计算每个点与所有聚类中心的距离并将其归入距离最近的一个类簇；
3. 计算每个类簇内所有点的均值作为该类簇新的聚类中心；
4. 重复上述步骤直到聚类中心不再变化或者变化不大。

- K-Means 的优点：计算简单，速度快；

- K-Means的缺点：
  1. 必须指定聚类数目K；
  2. 可能找到的是局部最优而非全局最优；
  3. 计算复杂度高；
  4. 容易受噪音点影响；

> - K-Means++改进了K-Means初始聚类中心的选择方法，首先从数据集中随机选取一个样本点作为初始聚类中心；接着计算每个样本与当前已有聚类中心之间的最短距离（即最近的聚类中心的距离），值越大被选择作为下一个聚类中心的概率越大；重复选取K个聚类中心。这样做能够让初始的聚类中心分布更加均匀。
>
> - K-Medians 用数据集的中位数而不是均值来计算数据的中心点。K-Medians 的优势是使用中位数来计算中心点不受异常值的影响；缺点是计算中位数时需要对数据集中的数据进行排序，速度相对于K-Means较慢。
>   
> - Mini Batch K-Means 适合在数据规模很大的时候使用，每次使用小批次数据进行K-Means更新聚类中心直到算法稳定。
